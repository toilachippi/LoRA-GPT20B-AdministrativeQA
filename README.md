# ğŸ§  Fine-tuning GPT-OSS 20B (Unsloth) cho dá»¯ liá»‡u Thá»§ tá»¥c hÃ nh chÃ­nh Viá»‡t Nam

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n quy trÃ¬nh chuáº©n Ä‘á»ƒ:
1. Chuáº©n bá»‹ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u há»™i thoáº¡i Ä‘á»‹nh dáº¡ng JSONL.  
2. Ãp dá»¥ng `tokenizer.apply_chat_template` Ä‘á»ƒ chuyá»ƒn dá»¯ liá»‡u thÃ nh chuá»—i huáº¥n luyá»‡n chuáº©n.  
3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng `SFTTrainer` (tá»« TRL) + Unsloth, **chá»‰ tÃ­nh loss trÃªn pháº§n tráº£ lá»i cá»§a assistant**.


## ğŸ“Œ 1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
pip install -U "transformers>=4.44" "accelerate>=0.34" "peft>=0.12" xformers
pip install -U bitsandbytes==0.43.3
pip install -U unsloth unsloth_zoo
```

> âš ï¸ TrÃ¡nh Ã©p phiÃªn báº£n `torch` vÃ  `triton` náº¿u khÃ´ng cháº¯c CUDA cá»§a mÃ¡y.
> `bitsandbytes` nÃªn khá»›p CUDA (vÃ­ dá»¥ Colab dÃ¹ng CUDA 12.x â†’ 0.43.3).

---

## ğŸ“Š 2. LÃ m sáº¡ch dá»¯ liá»‡u há»™i thoáº¡i

Dá»¯ liá»‡u gá»‘c cÃ³ dáº¡ng má»—i dÃ²ng JSON:

```json
{
  "messages": [
    {"role": "system", "content": "You are ChatGPT, ..."},
    {"role": "user", "content": "TÃ´i lÃ  1 ngÆ°á»i dÃ¢n Viá»‡t Nam ..."},
    {"role": "user", "content": "CÃ¡ nhÃ¢n Ä‘Äƒng kÃ½ ... cáº§n ná»™p nhá»¯ng loáº¡i giáº¥y tá» gÃ¬?"},
    {"role": "assistant", "content": "Theo quy Ä‘á»‹nh táº¡i Khoáº£n 3 ..."}
  ]
}
```

Ta cáº§n:

* Loáº¡i bá» metadata há»‡ thá»‘ng khÃ´ng cáº§n thiáº¿t.
* Giá»¯ láº¡i cáº¥u trÃºc há»™i thoáº¡i chuáº©n (system ngáº¯n gá»n, user, assistant).
* Táº¡o trÆ°á»ng `text` báº±ng `tokenizer.apply_chat_template`.

```python
from datasets import load_dataset

raw_ds = load_dataset(
    'json',
    data_files="/content/drive/MyDrive/Colab Notebooks/data_tthc/qa_conversations.jsonl",
    split='train'
)

def clean_messages(ex):
    msgs = []
    for m in ex["messages"]:
        role = m.get("role", "").strip()
        content = (m.get("content") or "").strip()
        if not role or not content:
            continue
        # Bá» cÃ¡c dÃ²ng meta khÃ´ng cáº§n thiáº¿t
        if role == "system" and ("Knowledge cutoff" in content or "Valid channels" in content):
            continue
        if role == "system":
            content = "Báº¡n lÃ  trá»£ lÃ½ thá»§ tá»¥c hÃ nh chÃ­nh cÃ´ng táº¡i Viá»‡t Nam."
        msgs.append({"role": role, "content": content})
    return {"messages": msgs}

cleaned = raw_ds.map(clean_messages)
```

---

## âœï¸ 3. Äá»‹nh dáº¡ng há»™i thoáº¡i báº±ng Chat Template

```python
def formatting_prompts_func(batch):
    texts = []
    for convo in batch["messages"]:
        s = tokenizer.apply_chat_template(
            convo,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(s)
    return {"text": texts}

formatted = cleaned.map(
    formatting_prompts_func,
    batched=True,
    remove_columns=cleaned.column_names
).filter(lambda ex: isinstance(ex["text"], str) and len(ex["text"]) > 0)

print(formatted[0]["text"][:300])
```

---

## ğŸ§ª 4. Cáº¥u hÃ¬nh Collator Ä‘á»ƒ tÃ­nh loss Ä‘Ãºng pháº§n assistant

Vá»›i template Unsloth, pháº§n tráº£ lá»i assistant báº¯t Ä‘áº§u báº±ng:

```
<|start|>assistant<|message|>
```

DÃ¹ng `DataCollatorForCompletionOnlyLM` Ä‘á»ƒ tá»± Ä‘á»™ng mask pháº§n prompt:

```python
from transformers import DataCollatorForCompletionOnlyLM

response_template = "<|start|>assistant<|message|>"
response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)

data_collator = DataCollatorForCompletionOnlyLM(
    response_template_ids=response_template_ids,
    tokenizer=tokenizer,
)
```

---

## ğŸš€ 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh

```python
from trl import SFTTrainer, SFTConfig

training_args = SFTConfig(
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 4,
    num_train_epochs = 1,
    learning_rate = 2e-4,
    logging_steps = 1,
    optim = 'adamw_8bit',
    weight_decay = 0.01,
    lr_scheduler_type = 'linear',
    seed = 3407,
    output_dir = 'outputs',
    report_to = 'none',
    max_seq_length = 1024,
    dataset_text_field = "text",
    packing = True,
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = formatted,
    args = training_args,
    data_collator = data_collator,
)

trainer.train()
```

---

## ğŸ§  6. Kiá»ƒm thá»­ mÃ´ hÃ¬nh sau huáº¥n luyá»‡n

```python
test_messages = [
    {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ thá»§ tá»¥c hÃ nh chÃ­nh cÃ´ng táº¡i Viá»‡t Nam."},
    {"role": "user", "content": "CÃ¡ nhÃ¢n Ä‘Äƒng kÃ½ Bá»“i dÆ°á»¡ng nghiá»‡p vá»¥ Ä‘Äƒng kiá»ƒm viÃªn tÃ u cÃ¡ cáº§n ná»™p giáº¥y tá» gÃ¬?"}
]

inputs = tokenizer.apply_chat_template(
    test_messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

output = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

---

## ğŸ“Œ Ghi chÃº

* Náº¿u cÃ³ nhiá»u lÆ°á»£t assistant trong má»™t máº«u, collator sáº½ tÃ­nh loss á»Ÿ táº¥t cáº£ pháº§n tráº£ lá»i.
* NÃªn Ä‘áº£m báº£o cÃ¡c trÆ°á»ng `messages` Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hoÃ¡ role (`system`, `user`, `assistant`) trÆ°á»›c khi mapping.
* CÃ³ thá»ƒ tÄƒng `num_train_epochs` hoáº·c `max_steps` khi huáº¥n luyá»‡n tháº­t.

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

* [Unsloth Documentation](https://github.com/unslothai/unsloth)
* [TRL - SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer)
* [DataCollatorForCompletionOnlyLM](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForCompletionOnlyLM)

---

## âœ¨ Káº¿t quáº£ mong Ä‘á»£i

Sau khi fine-tune, mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng:

* Hiá»ƒu ngá»¯ cáº£nh há»™i thoáº¡i hÃ nh chÃ­nh Viá»‡t Nam.
* Tráº£ lá»i chÃ­nh xÃ¡c theo quy Ä‘á»‹nh phÃ¡p lÃ½ Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p trong dá»¯ liá»‡u huáº¥n luyá»‡n.
* Giá»¯ Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng há»™i thoáº¡i chuáº©n `<|start|>user|assistant|>`.


